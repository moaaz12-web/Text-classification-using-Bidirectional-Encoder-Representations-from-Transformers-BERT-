{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING DEPENDENCIES"
      ],
      "metadata": {
        "id": "jG9jVdPdhx2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install transformers==4.0.0  Try this version of transformers if the latter version gives any errors\n",
        "# !pip install torch==1.4.0  Try this version of transformers if the former version of PyTorch gives any errors\n",
        "\n",
        "!pip install transformers==4.11.3\n",
        "!pip install 'lightning-flash[text]' --upgrade\n",
        "! pip install torch==1.5.0"
      ],
      "metadata": {
        "id": "1JorwRHAYTF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING LIBRARIES AND DATASETS\n"
      ],
      "metadata": {
        "id": "5-pHk5ngh0gq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alVhUndQX7YB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Check pytorch version\n",
        "print(torch.__version__) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_fwf(r'/content/drive/MyDrive/AI/train_labelled.txt', header=None,encoding=\"ISO-8859-1\",\n",
        "                 error_bad_lines=False, usecols=[0,1])\n",
        "\n",
        "df_test = pd.read_fwf(r'/content/drive/MyDrive/AI/val_labell.txt', header=None,encoding=\"ISO-8859-1\",\n",
        "                 error_bad_lines=False, usecols=[0,1])\n"
      ],
      "metadata": {
        "id": "LezDDNvfbucZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "0s8s0hWgh40W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RENAMING COLUMNS"
      ],
      "metadata": {
        "id": "bgRi7cw0iG3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={0: 'Sentiment',\n",
        "                   1: 'Text'},\n",
        "          inplace=True, errors='raise')\n",
        "\n",
        "df_test.rename(columns={0: 'Sentiment',\n",
        "                   1: 'Text'},\n",
        "          inplace=True, errors='raise')"
      ],
      "metadata": {
        "id": "KK_WcVbdcj97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DOING SOME REGEX FOR TEXT CLEANING"
      ],
      "metadata": {
        "id": "iEcmVfkOiJM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# remove all links\n",
        "df['Text'] = df['Text'].str.replace(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', regex=True)\n",
        "\n",
        "# remove all line breaks \n",
        "df['Text'] = df['Text'].str.replace(r'\\n', '', regex=True)    \n",
        "# text['tweet_text'] = text['tweet_text'].str.replace(r'\\t', '', regex=True)\n",
        "df['Text'] = df['Text'].str.replace(r'\\r', '', regex=True)\n",
        "\n",
        "# remove all emojis\n",
        "df['Text'] = df['Text'].str.replace(r\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", '', flags=re.UNICODE,regex=True)\n",
        "\n",
        "\n",
        "# Remove double fullstops\n",
        "df['Text'] = df['Text'].str.replace(r'\\.+', '.', regex=True)\n",
        "\n",
        "\n",
        "df['Text'] = df['Text'].str.replace(r\"\\.(?=\\S)\", '.', regex=True)\n",
        "\n",
        "#Remove &amp sign\n",
        "df['Text'] =df['Text'].str.replace(r\"&amp;\", 'and', regex=True)\n",
        "\n",
        "df['Text'] = df['Text'].str.replace(r'/\\S*@\\S*\\s?/g', '', regex=True)# remove all emails\n",
        "\n",
        "# Remove all usernames from the tweets column for better interpretability, AFTER ADDING TO ANOTHER COLUMN\n",
        "df['Text'] = df['Text'].str.replace(r'([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)', '', regex=True)  # remove all emails\n",
        "\n",
        "\n",
        "# Instead of the upper two regexes, lets use this. Itll keep the user names but just remove the @ symbol\n",
        "df['Text'] = df['Text'].str.replace('@', '', regex=True)\n",
        "df['Text']=df['Text'].str.capitalize()\n",
        "\n",
        "\n",
        "# remove all links\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', regex=True)\n",
        "\n",
        "# remove all line breaks \n",
        "df_test['Text'] = df_test['Text'].str.replace(r'\\n', '', regex=True)    \n",
        "# text['tweet_text'] = text['tweet_text'].str.replace(r'\\t', '', regex=True)\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'\\r', '', regex=True)\n",
        "\n",
        "# remove all emojis\n",
        "df_test['Text'] = df_test['Text'].str.replace(r\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", '', flags=re.UNICODE,regex=True)\n",
        "\n",
        "\n",
        "# Remove double fullstops\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'\\.+', '.', regex=True)\n",
        "\n",
        "df_test['Text'] = df_test['Text'].str.replace(r\"\\.(?=\\S)\", '.', regex=True)\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'\\â€”', 'and', regex=True)\n",
        "\n",
        "\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'/\\S*@\\S*\\s?/g', '', regex=True)  # remove all emails\n",
        "\n",
        "# Remove all usernames from the tweets column for better interpretability, AFTER ADDING TO ANOTHER COLUMN\n",
        "df_test['Text'] = df_test['Text'].str.replace(r'([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)', '', regex=True)  # remove all emails\n",
        "\n",
        "\n",
        "# Instead of the upper two regexes, lets use this. Itll keep the user names but just remove the @ symbol\n",
        "df_test['Text'] = df_test['Text'].str.replace('@', '', regex=True)\n",
        "df_test['Text']=df_test['Text'].str.capitalize()\n"
      ],
      "metadata": {
        "id": "xAul6yS9eSJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Sentiment.unique()"
      ],
      "metadata": {
        "id": "T6I8d3OwfcIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RENAMING LABELS"
      ],
      "metadata": {
        "id": "IMVH_IeQiNdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'] = df['Sentiment'].replace(['__label__1','__label__2', '__label__3', '__label__4', '__label__5'],\n",
        "                                              [0,1,2,3,4])\n",
        "\n",
        "df_test['Sentiment'] = df_test['Sentiment'].replace(['__label__1','__label__2', '__label__3', '__label__4', '__label__5'],\n",
        "                                              [0,1,2,3,4])\n"
      ],
      "metadata": {
        "id": "-4GjYQ0rfdNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  SPLITTING DATA INTO TRAINING AND TEST"
      ],
      "metadata": {
        "id": "Qgz8GD4JiQGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = list(df['Text'])\n",
        "X_test = list(df_test['Text'])\n",
        "y_train = list(df['Sentiment'])\n",
        "y_test = list(df_test['Sentiment'])"
      ],
      "metadata": {
        "id": "OmUR27OfgQV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING THE TOKENIZER AND INSTANTIATING TRANSFORMER BERT MODEL"
      ],
      "metadata": {
        "id": "iMmvDAeYiTUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=5)"
      ],
      "metadata": {
        "id": "j0sdn6ChneoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')\n"
      ],
      "metadata": {
        "id": "zr9Se11Ut1wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT TOKENIZATION (NECESSARY TO TRANSFORM TEXT TO NUMBERS)"
      ],
      "metadata": {
        "id": "2xPt9I04isHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)"
      ],
      "metadata": {
        "id": "6YhblwCYpEgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONVERTING TEXT AND LABELS TO TENSORS"
      ],
      "metadata": {
        "id": "T_F6sXd0ix7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "TWNoq7jzogWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_test)\n"
      ],
      "metadata": {
        "id": "1DDwE_lBojcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING A COMPUTE FUNCTION THAT WILL CALCULATE THE MODEL ACCURACY, PRECISION, RECALL, AND F1-SCORE"
      ],
      "metadata": {
        "id": "CZoygmLji4Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check these functions too, if u want. Otherwise, simply delete this cell!\n",
        "# def compute_metrics(p):\n",
        "#     print(type(p))\n",
        "#     pred, labels = p\n",
        "#     pred = np.argmax(pred, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "#     recall = recall_score(y_true=labels, y_pred=pred)\n",
        "#     precision = precision_score(y_true=labels, y_pred=pred)\n",
        "#     f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "#     return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "# import numpy as np\n",
        "# import evaluate\n",
        "#  metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     predictions = np.argmax(logits, axis=-1)\n",
        "#     return metric.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "6MyRTSmXCNrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "f4N0D9E4opXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING THE TRAINER USING THE TRAINER API"
      ],
      "metadata": {
        "id": "JM1LxUXZi_rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=16,\n",
        "    \n",
        "\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "lB2XHAiHi_Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING"
      ],
      "metadata": {
        "id": "n5y9GvyVjDNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "AFkktvnxqdlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATE THE MODEL"
      ],
      "metadata": {
        "id": "rfct6XQvjKoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install evaluate"
      ],
      "metadata": {
        "id": "SluurDR97lOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is causing some errors right now, ill fix it!\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Hiz1KRO7jIgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE THE MODEL"
      ],
      "metadata": {
        "id": "93GAer2CjE1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('custom_modelgggglatest')\n"
      ],
      "metadata": {
        "id": "r4Rjyq5CnSN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAKE PREDICTIONS"
      ],
      "metadata": {
        "id": "3FgnPjBljNww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write in the text\n"
      ],
      "metadata": {
        "id": "WPbAhTuGsv1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"That was good point\""
      ],
      "metadata": {
        "id": "SN69CQjcnBvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the text and send it to the model"
      ],
      "metadata": {
        "id": "Q0ZebuP7s1FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
        "outputs = model(**inputs)\n",
        "# print(outputs)"
      ],
      "metadata": {
        "id": "Eth3H7pkmIDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the predictions for each labels, representing probabilities. Remember: \n",
        "\n",
        "__label__1 --> 0\n",
        "<br></br>\n",
        "\n",
        "__label__2 --> 1 \n",
        "<br></br>\n",
        "\n",
        "__label__3 --> 2,\n",
        "<br></br>\n",
        "\n",
        "__label__4 --> 3,\n",
        "<br></br>\n",
        "\n",
        "__label__5 --> 4\n",
        "<br></br>\n",
        "\n",
        "                                            "
      ],
      "metadata": {
        "id": "KLXE_y_Ls5mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "Xnevh9VSl7oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predictions.cpu().detach().numpy()\n",
        "predictions"
      ],
      "metadata": {
        "id": "lCS1gK6vtl7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the prediction with the highest probabilitly\n",
        "\n"
      ],
      "metadata": {
        "id": "wjTMshKZtfzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Probability is\" , predictions[0][np.argmax(predictions)]*100 , \" belonging to class \" , np.argmax(predictions))"
      ],
      "metadata": {
        "id": "Zs7fc0rtsTBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGvl8XNqt07e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}